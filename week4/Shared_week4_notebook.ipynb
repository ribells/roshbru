{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rosha Week 2 code example (annotating images for facial components recognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /var/folders/zf/49prmn_s7s5861rytkmr3_080000gn/T/matplotlib-s8pq61f6 because the default path (/Users/ribells/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "2024-08-13 08:43:54.747111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nose tip:\n",
      "x: 0.32838702\n",
      "y: 0.18372527\n",
      "\n",
      "Annotated image saved to tmp/annotated_image0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize the face detection and drawing utilities\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# List of image files to process\n",
    "IMAGE_FILES = [r'img1.jpg']\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = r'tmp'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set up the face detection model\n",
    "with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "    # Loop through each image file\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        # Read the image\n",
    "        image = cv2.imread(file)\n",
    "        # Check if the image was read successfully\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image file {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert the BGR image to RGB and process it with MediaPipe Face Detection\n",
    "        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # If no faces are detected, continue to the next image\n",
    "        if not results.detections:\n",
    "            continue\n",
    "        \n",
    "        # Create a copy of the image for annotations\n",
    "        annotated_image = image.copy()\n",
    "        \n",
    "        # Loop through each detected face\n",
    "        for detection in results.detections:\n",
    "            # Print the nose tip coordinates\n",
    "            print('Nose tip:')\n",
    "            print(mp_face_detection.get_key_point(\n",
    "                detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "            # Draw the detection annotations on the image\n",
    "            mp_drawing.draw_detection(annotated_image, detection)\n",
    "        \n",
    "        # Save the annotated image to a file\n",
    "        output_file = os.path.join(output_dir, 'annotated_image' + str(idx) + '.png')\n",
    "        cv2.imwrite(output_file, annotated_image)\n",
    "        print(f\"Annotated image saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple example of facial detection during live video stream\n",
    "\n",
    "This next two cells show you how to use MediaPipe Tasks Python API to detect facial components in a video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /var/folders/zf/49prmn_s7s5861rytkmr3_080000gn/T/matplotlib-2udy_tky because the default path (/Users/ribells/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "2024-07-01 10:11:21.628142: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Draw face detections of each face.\n",
    "    if not results.detections:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for detection in results.detections:\n",
    "      print('Nose tip:')\n",
    "      print(mp_face_detction.get_key_point(\n",
    "          detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "      mp_drawing.draw_detection(annotated_image, detection)\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.detections:\n",
    "      for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# 3. Pose Landmarks Detection with MediaPipe Tasks\n",
    "\n",
    "This notebook shows you how to use MediaPipe Tasks Python API to detect pose landmarks from images.\n",
    "\n",
    "https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6PN9FvIx614"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start with installing MediaPipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gxbHBsF-8Y_l"
   },
   "outputs": [],
   "source": [
    "#Note: install successful - only needed to run once\n",
    "#!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49D7h4TVmru"
   },
   "source": [
    "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models) for more information about this model bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OMjuVQiDYJKF"
   },
   "outputs": [],
   "source": [
    "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYKAJ5nDU8-I"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "s3E6NFV-00Qt"
   },
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83PEJNp9yPBU"
   },
   "source": [
    "## Download test image\n",
    "\n",
    "To demonstrate the Pose Landmarker API, you can download a sample image using the follow code. The image is from [Pixabay](https://pixabay.com/photos/girl-woman-fitness-beautiful-smile-4051811/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tzXuqyIBlXer"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!wget -q -O image.jpg https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg\n",
    "\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "#img = cv2.imread(\"image.jpg\")\n",
    "im = Image.open(\"image.jpg\")\n",
    "px = im.load()\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-skLwMBmMN_"
   },
   "source": [
    "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etBjSdwImQPw"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# if len(uploaded.keys()):\n",
    "#   IMAGE_FILE = next(iter(uploaded))\n",
    "#   print('Uploaded file:', IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4r2_ePylIa"
   },
   "source": [
    "## Running inference and visualizing the results\n",
    "\n",
    "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
    "\n",
    "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python) to learn more about configuration options that this solution supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_JVO3rvPD4RN"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe.tasks.python.vision' has no attribute 'PoseLandmarkerOptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# STEP 2: Create an PoseLandmarker object.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_landmarker.task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoseLandmarkerOptions\u001b[49m(\n\u001b[1;32m      9\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mbase_options,\n\u001b[1;32m     10\u001b[0m     output_segmentation_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m detector \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mPoseLandmarker\u001b[38;5;241m.\u001b[39mcreate_from_options(options)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# STEP 3: Load the input image.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'mediapipe.tasks.python.vision' has no attribute 'PoseLandmarkerOptions'"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"image.jpg\")\n",
    "\n",
    "# STEP 4: Detect pose landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: PoseLandmarkerOptions not available with Mac OSX version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BwzFvaxwtPX"
   },
   "source": [
    "Visualize the pose segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jAIFzw9M3JJ"
   },
   "outputs": [],
   "source": [
    "segmentation_mask = detection_result.segmentation_masks[0].numpy_view()\n",
    "visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255\n",
    "cv2_imshow(visualized_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mediapipe Face Mesh Example\n",
    "\n",
    "https://chuoling.github.io/mediapipe/solutions/face_mesh.html#mediapipe-face-mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print and draw face mesh landmarks on the image.\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "      print('face_landmarks:', face_landmarks)\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_tesselation_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_contours_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n",
    "\n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    # Draw the face mesh annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "      for face_landmarks in results.multi_face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles\n",
    "            .get_default_face_mesh_iris_connections_style())\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we want to use our own customized facemesh instead of the default Mediapipe one \n",
    "\n",
    "although the default has 468 mesh points\n",
    "\n",
    "Here's a video to consider:\n",
    "https://www.youtube.com/watch?v=LGPBRH6Hqw8\n",
    "\n",
    "And here is the Mediapipe face mesh documentation:\n",
    "https://github.com/google-ai-edge/mediapipe/wiki/MediaPipe-Face-Mesh\n",
    "\n",
    "Found a discussion about custom mesh, attempted to follow...\n",
    "https://github.com/google-ai-edge/mediapipe/issues/1155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Kanstantsin Sokal.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Converts OBJ file into `mediapipe.face_geometry.Mesh3d`.\n",
    "#\n",
    "# The OBJ file must be located in the same directory and named \"model.obj\".\n",
    "# The output protobuf test will be written into \"model.pbtxt\".\n",
    "def Main():\n",
    "  file_in = open(\"model.obj\", \"r\")\n",
    "  file_out = open(\"model.pbtxt\", \"w\")\n",
    "\n",
    "  v_positions = []\n",
    "  v_tex_coords = []\n",
    "  faces = []\n",
    "\n",
    "  compressed_v_ids = {}\n",
    "  compressed_vs = []\n",
    "\n",
    "  for line in file_in.readlines():\n",
    "    tokens = line.strip().split(' ')\n",
    "\n",
    "    if tokens[0] == \"v\":\n",
    "      v_positions.append((float(tokens[1]), float(tokens[2]), float(tokens[3])))\n",
    "    elif tokens[0] == \"vt\":\n",
    "      v_tex_coords.append((float(tokens[1]), float(tokens[2])))\n",
    "    elif tokens[0] == \"f\":\n",
    "      v_position_ids = [int(token.split('/')[0]) - 1 for token in tokens[1:]]\n",
    "      v_tex_coord_ids = [int(token.split('/')[1]) - 1 for token in tokens[1:]]\n",
    "      v_compressed_ids = []\n",
    "\n",
    "      for v_position_id, v_tex_coord_id in zip(v_position_ids, v_tex_coord_ids):\n",
    "        key = (v_position_id, v_tex_coord_id)\n",
    "        if key not in compressed_v_ids:\n",
    "          compressed_v_ids[key] = len(compressed_vs)\n",
    "          compressed_vs.append(key)\n",
    "\n",
    "        v_compressed_ids.append(compressed_v_ids[key])\n",
    "\n",
    "      for middle_id in range(1, len(v_compressed_ids) - 1):\n",
    "        face = []\n",
    "        face.append(v_compressed_ids[0])\n",
    "        face.append(v_compressed_ids[middle_id])\n",
    "        face.append(v_compressed_ids[middle_id + 1])\n",
    "        faces.append(tuple(face))\n",
    "\n",
    "  file_out.write(\"vertex_type: VERTEX_PT\\n\")\n",
    "  file_out.write(\"primitive_type: TRIANGLE\\n\")\n",
    "\n",
    "  for v_position_id, v_tex_coord_id in compressed_vs:\n",
    "    v_position = v_positions[v_position_id]\n",
    "    v_tex_coord = v_tex_coords[v_tex_coord_id]\n",
    "\n",
    "    line = \"\"\n",
    "    line += \"vertex_buffer: {:.6f}\\n\".format(v_position[0])\n",
    "    line += \"vertex_buffer: {:.6f}\\n\".format(v_position[1])\n",
    "    line += \"vertex_buffer: {:.6f}\\n\".format(v_position[2])\n",
    "    line += \"vertex_buffer: {:.6f}\\n\".format(v_tex_coord[0])\n",
    "    line += \"vertex_buffer: {:.6f}\\n\".format(1 - v_tex_coord[1])\n",
    "    file_out.write(line)\n",
    "\n",
    "  for face in faces:\n",
    "    line = \"\"\n",
    "    line += \"index_buffer: {}\\n\".format(face[0])\n",
    "    line += \"index_buffer: {}\\n\".format(face[1])\n",
    "    line += \"index_buffer: {}\\n\".format(face[2])\n",
    "    file_out.write(line)\n",
    "\n",
    "  file_in.close()\n",
    "  file_out.close()\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mesh_3d_pb2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_format\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext_format\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmesh_3d_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mesh3d\n\u001b[0;32m      4\u001b[0m message \u001b[38;5;241m=\u001b[39m Mesh3d()  \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pbtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mesh_3d_pb2'"
     ]
    }
   ],
   "source": [
    "import google.protobuf.text_format as text_format\n",
    "from mesh_3d_pb2 import Mesh3d\n",
    "\n",
    "message = Mesh3d()  \n",
    "\n",
    "with open('model.pbtxt', 'r') as file:\n",
    "    text_format.Merge(file.read(), message)\n",
    "\n",
    "# Convert to binary and save to a new .pbbinary file\n",
    "with open('model.pbbinary', 'wb') as file:\n",
    "    file.write(message.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
