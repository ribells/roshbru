{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple example of facial detection during live video stream\n",
    "\n",
    "This next two cells show you how to use MediaPipe Tasks Python API to detect facial components in a video stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Draw face detections of each face.\n",
    "    if not results.detections:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for detection in results.detections:\n",
    "      print('Nose tip:')\n",
    "      print(mp_face_detction.get_key_point(\n",
    "          detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))\n",
    "      mp_drawing.draw_detection(annotated_image, detection)\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.detections:\n",
    "      for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Face Detection with Mediapipe\n",
    "\n",
    "https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe\n",
    "import cv2\n",
    "\n",
    "#https://www.pexels.com/photo/portrait-photo-of-woman-in-white-crew-neck-shirt-8090149/\n",
    "img = cv2.imread(\"face-example.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mediapipe.solutions.face_detection\n",
    "face_detector =  mp_face_detection.FaceDetection( min_detection_confidence = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = face_detector.process(img)\n",
    "\n",
    "if results.detections:\n",
    "    for face in results.detections:\n",
    "        confidence = face.score\n",
    "        bounding_box = face.location_data.relative_bounding_box\n",
    "         \n",
    "        x = int(bounding_box.xmin * img.shape[1])\n",
    "        w = int(bounding_box.width * img.shape[1])\n",
    "        y = int(bounding_box.ymin * img.shape[0])\n",
    "        h = int(bounding_box.height * img.shape[0])\n",
    "         \n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 255), thickness = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = face.location_data.relative_keypoints\n",
    " \n",
    "right_eye = (int(landmarks[0].x * img.shape[1]), int(landmarks[0].y * img.shape[0]))\n",
    "left_eye = (int(landmarks[1].x * img.shape[1]), int(landmarks[1].y * img.shape[0]))\n",
    "nose = (int(landmarks[2].x * img.shape[1]), int(landmarks[2].y * img.shape[0]))\n",
    "mouth = (int(landmarks[3].x * img.shape[1]), int(landmarks[3].y * img.shape[0]))\n",
    "right_ear = (int(landmarks[4].x * img.shape[1]), int(landmarks[4].y * img.shape[0]))\n",
    "left_ear = (int(landmarks[5].x * img.shape[1]), int(landmarks[5].y * img.shape[0]))\n",
    " \n",
    "cv2.circle(img, right_eye, 15, (0, 0, 255), -1)\n",
    "cv2.circle(img, left_eye, 15, (0, 0, 255), -1)\n",
    "cv2.circle(img, nose, 15, (0, 0, 255), -1)\n",
    "cv2.circle(img, mouth, 15, (0, 0, 255), -1)\n",
    "cv2.circle(img, right_ear, 15, (0, 0, 255), -1)\n",
    "cv2.circle(img, left_ear, 15, (0, 0, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 10:10:51.823024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have tensorflow 2.16.1 and this requires tf-keras package. Please run `pip install tf-keras` or downgrade your tensorflow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/retinaface/commons/package_utils.py:19\u001b[0m, in \u001b[0;36mvalidate_for_keras3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras is already available - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf_keras\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#face detection\u001b[39;00m\n\u001b[1;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m DeepFace\u001b[38;5;241m.\u001b[39mdetectFace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, detector_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmediapipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/DeepFace.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m log\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     modeling,\n\u001b[1;32m     22\u001b[0m     representation,\n\u001b[1;32m     23\u001b[0m     verification,\n\u001b[1;32m     24\u001b[0m     recognition,\n\u001b[1;32m     25\u001b[0m     demography,\n\u001b[1;32m     26\u001b[0m     detection,\n\u001b[1;32m     27\u001b[0m     streaming,\n\u001b[1;32m     28\u001b[0m     preprocessing,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     32\u001b[0m logger \u001b[38;5;241m=\u001b[39m log\u001b[38;5;241m.\u001b[39mget_singletonish_logger()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/modules/modeling.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasemodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     VGGFace,\n\u001b[1;32m      7\u001b[0m     OpenFace,\n\u001b[1;32m      8\u001b[0m     FbDeepFace,\n\u001b[1;32m      9\u001b[0m     DeepID,\n\u001b[1;32m     10\u001b[0m     ArcFace,\n\u001b[1;32m     11\u001b[0m     SFace,\n\u001b[1;32m     12\u001b[0m     Dlib,\n\u001b[1;32m     13\u001b[0m     Facenet,\n\u001b[1;32m     14\u001b[0m     GhostFaceNet,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextendedmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Age, Gender, Race, Emotion\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspoofmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FasNet\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/basemodels/VGGFace.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m verification\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacialRecognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacialRecognition\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m log\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/modules/verification.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m representation, detection, modeling\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacialRecognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacialRecognition\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m log\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/modules/representation.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_utils\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modeling, detection, preprocessing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacialRecognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacialRecognition\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresent\u001b[39m(\n\u001b[1;32m     14\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[1;32m     15\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG-Face\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     anti_spoofing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/modules/detection.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modeling\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectedFace, FacialAreaRegion\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectorWrapper\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m log\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/detectors/DetectorWrapper.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detection\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Detector, DetectedFace, FacialAreaRegion\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     FastMtCnn,\n\u001b[1;32m      8\u001b[0m     MediaPipe,\n\u001b[1;32m      9\u001b[0m     MtCnn,\n\u001b[1;32m     10\u001b[0m     OpenCv,\n\u001b[1;32m     11\u001b[0m     Dlib,\n\u001b[1;32m     12\u001b[0m     RetinaFace,\n\u001b[1;32m     13\u001b[0m     Ssd,\n\u001b[1;32m     14\u001b[0m     Yolo,\n\u001b[1;32m     15\u001b[0m     YuNet,\n\u001b[1;32m     16\u001b[0m     CenterFace,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger \u001b[38;5;28;01mas\u001b[39;00m log\n\u001b[1;32m     20\u001b[0m logger \u001b[38;5;241m=\u001b[39m log\u001b[38;5;241m.\u001b[39mget_singletonish_logger()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/deepface/detectors/RetinaFace.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretinaface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetinaFace \u001b[38;5;28;01mas\u001b[39;00m rf\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDetector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Detector, FacialAreaRegion\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# pylint: disable=too-few-public-methods\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/retinaface/RetinaFace.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mretinaface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# users should install tf_keras package if they are using tf 2.16 or later versions\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mpackage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_for_keras3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretinaface/RetinaFace.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: disable=global-variable-undefined, no-name-in-module, unused-import, too-many-locals, redefined-outer-name, too-many-statements, too-many-arguments\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# configurations\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/retinaface/commons/package_utils.py:24\u001b[0m, in \u001b[0;36mvalidate_for_keras3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_keras is already available - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf_keras\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# you may consider to install that package here\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have tensorflow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and this requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf-keras package. Please run `pip install tf-keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor downgrade your tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You have tensorflow 2.16.1 and this requires tf-keras package. Please run `pip install tf-keras` or downgrade your tensorflow."
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    " \n",
    "#face detection\n",
    "img = DeepFace.detectFace(\"img.jpg\", detector_backend = \"mediapipe\")\n",
    " \n",
    "#face verification\n",
    "obj = DeepFace.verify(img1_path = \"img1.jpg\", img2_path = \"img2.jpg\", detector_backend = \"mediapipe\")\n",
    "print(obj[\"verified\"])\n",
    " \n",
    "#face recognition\n",
    "df = DeepFace.find(img_path = \"img.jpg\", db_path = \"my_db\", detector_backend =  \"mediapipe\")\n",
    "print(df.head())\n",
    " \n",
    "#facial analysis\n",
    "demography = DeepFace.analyze(img_path = \"img4.jpg\", detector_backend =  \"mediapipe\")\n",
    "print(demography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# 3. Pose Landmarks Detection with MediaPipe Tasks\n",
    "\n",
    "This notebook shows you how to use MediaPipe Tasks Python API to detect pose landmarks from images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6PN9FvIx614"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start with installing MediaPipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gxbHBsF-8Y_l"
   },
   "outputs": [],
   "source": [
    "#Note: install successful - only needed to run once\n",
    "#!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49D7h4TVmru"
   },
   "source": [
    "Then download an off-the-shelf model bundle. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker#models) for more information about this model bundle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OMjuVQiDYJKF"
   },
   "outputs": [],
   "source": [
    "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYKAJ5nDU8-I"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "s3E6NFV-00Qt"
   },
   "outputs": [],
   "source": [
    "#@markdown To better demonstrate the Pose Landmarker API, we have created a set of visualization tools that will be used in this colab. These will draw the landmarks on a detect person, as well as the expected connections between those markers.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83PEJNp9yPBU"
   },
   "source": [
    "## Download test image\n",
    "\n",
    "To demonstrate the Pose Landmarker API, you can download a sample image using the follow code. The image is from [Pixabay](https://pixabay.com/photos/girl-woman-fitness-beautiful-smile-4051811/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tzXuqyIBlXer"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!wget -q -O image.jpg https://cdn.pixabay.com/photo/2019/03/12/20/39/girl-4051811_960_720.jpg\n",
    "\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "#img = cv2.imread(\"image.jpg\")\n",
    "im = Image.open(\"image.jpg\")\n",
    "px = im.load()\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-skLwMBmMN_"
   },
   "source": [
    "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etBjSdwImQPw"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for filename in uploaded:\n",
    "#   content = uploaded[filename]\n",
    "#   with open(filename, 'wb') as f:\n",
    "#     f.write(content)\n",
    "\n",
    "# if len(uploaded.keys()):\n",
    "#   IMAGE_FILE = next(iter(uploaded))\n",
    "#   print('Uploaded file:', IMAGE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4r2_ePylIa"
   },
   "source": [
    "## Running inference and visualizing the results\n",
    "\n",
    "The final step is to run pose landmark detection on your selected image. This involves creating your PoseLandmarker object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
    "\n",
    "Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python) to learn more about configuration options that this solution supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_JVO3rvPD4RN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# STEP 2: Create an PoseLandmarker object.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_landmarker.task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[43mtasks\u001b[49m\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mPoseLandmarkerOptions(\n\u001b[1;32m      9\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mbase_options,\n\u001b[1;32m     10\u001b[0m     output_segmentation_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m detector \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mPoseLandmarker\u001b[38;5;241m.\u001b[39mcreate_from_options(options)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# STEP 3: Load the input image.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tasks' is not defined"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"image.jpg\")\n",
    "\n",
    "# STEP 4: Detect pose landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BwzFvaxwtPX"
   },
   "source": [
    "Visualize the pose segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jAIFzw9M3JJ"
   },
   "outputs": [],
   "source": [
    "segmentation_mask = detection_result.segmentation_masks[0].numpy_view()\n",
    "visualized_mask = np.repeat(segmentation_mask[:, :, np.newaxis], 3, axis=2) * 255\n",
    "cv2_imshow(visualized_mask)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
